
# ğŸ›ï¸ E-Commerce Product Scraper & ETL Pipeline

Scrape â€¢ Clean â€¢ Transform â€¢ Load  
ğŸ“¦ Built with Python, BeautifulSoup, Pandas, SQLAlchemy

---

## ğŸš€ Overview

This project automates the process of extracting product data from a simulated e-commerce website, transforming it into a clean format, and loading it into a SQL database. It's designed to simulate a real-world **ETL (Extract, Transform, Load)** pipeline and supports scalable scraping and structured storage for downstream analysis.

---

## ğŸ“Œ Features

- âœ… **Robust Web Scraping** using BeautifulSoup
- ğŸ” Smart error handling for inconsistent HTML structures (like missing prices or ratings)
- ğŸ§¹ Data Cleaning & Transformation
- ğŸ§ª Unit Tested Functions with Code Coverage Reporting
- ğŸ›¢ï¸ SQL-ready output using Pandas and SQLAlchemy
- â±ï¸ Auto-timestamping for data tracking

---

## ğŸ§± Project Structure

```
ğŸ“¦ project-root/
â”œâ”€â”€ utils/
â”‚   â”œâ”€â”€ extract.py         # Parses HTML and extracts data
â”‚   â”œâ”€â”€ transform.py       # Cleans and transforms raw data
â”‚   â”œâ”€â”€ load.py            # Loads data into SQL DB
â”œâ”€â”€ test/                  # Unit tests
â”‚   â”œâ”€â”€ test_extract.py    
|   â”œâ”€â”€ test_load.py
|   â”œâ”€â”€ test_transform.py
â”œâ”€â”€ main.py                # Main orchestration script
â”œâ”€â”€ requirements.txt       # Python dependencies
â””â”€â”€ README.md              # Project documentation
```

---

## ğŸ› ï¸ Setup & Installation

1. **Clone the repository**
```bash
git clone https://github.com/yourusername/ecommerce-scraper-etl.git
cd ecommerce-scraper-etl
```

2. **Install dependencies**
```bash
pip install -r requirements.txt
```

3. **Run the pipeline**
```bash
python main.py
```

---

## ğŸ§ª Testing

This project includes unit tests for core components.  
To run tests and see code coverage:

```bash
python -m pytest tests
pytest --cov=utils --cov-report=term-missing tests
```

---

## ğŸ§© Example Output

Here's an example of the structured output saved to the database:

| title             | price   | rating           | colors    | size | gender | timestamp           |
|------------------|---------|------------------|-----------|------|--------|---------------------|
| Unknown Product  | $100.00 | Invalid Rating   | 5 Colors  | M    | Men    | 2025-05-15 11:00:00 |
| Pants 46         | None    | Not Rated        | 8 Colors  | S    | Men    | 2025-05-15 11:00:01 |

---

## ğŸ“š Technologies Used

- Python 3.10+
- BeautifulSoup4
- Requests
- Pandas
- SQLAlchemy
- SQLite (default, can be swapped with PostgreSQL/MySQL)
- Pytest

---

## ğŸ’¡ Future Improvements

- Asynchronous scraping with `aiohttp` for speed
- Integration with cloud-based DBs (e.g., Google BigQuery)
- Auto-scheduling with Airflow or cron
- Export to CSV/Excel

---

## Notes!
I deleted the google-sheets-api.json because it contain a secret key of my Google Console API Service. In this repository, you will also only find a load function to a CSV file, but actually I did load to 3 types of data repository.

---

## Accepted Submission Evidence
![Accepted Submission](accepted-submission.png)

---

## ğŸ§‘â€ğŸ’» Author

Created with â¤ï¸ for the **DBS Coding Camp 2025**  
Feel free to fork, contribute, or suggest improvements!
